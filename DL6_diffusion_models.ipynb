{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapshinaaa/deep-learning-tasks/blob/main/DL6_diffusion_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68oNOYBFLY_x"
      },
      "source": [
        "# `Diffusion Models`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1i68n_LY_y"
      },
      "source": [
        "**Assignment Plan**\n",
        "\n",
        "We will implement our own diffusion model with experiment logging and first test it on Swiss Roll data, then run it on MNIST.\n",
        "\n",
        "**Goal of the Assignment**\n",
        "\n",
        "The goal of this assignment is to understand how diffusion models work and to carefully go through the underlying mathematical formulas.  \n",
        "At the same time, training difficulties should be minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3npoJHNSskn"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQQBejTGSskn"
      },
      "source": [
        "As an initial task, you need to extend the provided code with experiment logging using  \n",
        "`wandb` or any other similar logging service.\n",
        "\n",
        "### What can be logged\n",
        "- Main hyperparameters (learning rate, batch size, number of diffusion steps, diffusion schedule, etc.)\n",
        "- Loss values\n",
        "- Gradient norms\n",
        "- Model parameters and architecture\n",
        "- Weight distributions\n",
        "- Generated samples\n",
        "- Hardware characteristics  \n",
        "In general â€” log everything you can reasonably log."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcHHLrnuLY_0"
      },
      "source": [
        "## Imports and SwissRolls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCEuXFYOV-Fy"
      },
      "outputs": [],
      "source": [
        "!wget --quiet --show-progress \"https://raw.githubusercontent.com/yandexdataschool/deep_vision_and_graphics/fall24/homework04/utils.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcjQMkRkLY_1"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEZ1oA4BLY_2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbi3yqCBWHT5"
      },
      "outputs": [],
      "source": [
        "SEED = 777\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NeWUCL0LY_3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles, make_swiss_roll\n",
        "\n",
        "\n",
        "def make_swiss_dataset(num_samples):\n",
        "    X0, _ = make_swiss_roll(num_samples // 2, noise=0.3, random_state=0)\n",
        "    X1, _ = make_swiss_roll(num_samples // 2, noise=0.3, random_state=0)\n",
        "    X0 = X0[:, [0, 2]]\n",
        "    X1 = X1[:, [0, 2]]\n",
        "    X1 = -X1\n",
        "    X, y = shuffle(\n",
        "        np.concatenate([X0, X1], axis=0),\n",
        "        np.concatenate([np.zeros(len(X0)), np.ones(len(X1))], axis=0),\n",
        "        random_state=0,\n",
        "    )\n",
        "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = make_swiss_dataset(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auZANvzMLY_5"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uncDovU4LY_6"
      },
      "source": [
        "## DDPM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWqy7MvWLY_8"
      },
      "source": [
        "In this part, you will implement your own diffusion model (DDPM) and test it on the dataset described above.\n",
        "\n",
        "## Reminder\n",
        "\n",
        "Recall that a diffusion model consists of a forward process and a reverse process.\n",
        "\n",
        "### Forward Diffusion Process\n",
        "\n",
        "The forward diffusion process is defined as the posterior distribution  \n",
        "$q(x_{1:T} \\mid x_0)$.  \n",
        "This distribution is a Markov chain that gradually adds Gaussian noise to an initial object $x_0$.\n",
        "\n",
        "At each step, noise is added with a different magnitude determined by the variance schedule\n",
        "$\\{\\beta_1, \\dots, \\beta_T\\}$.\n",
        "\n",
        "With a proper choice of the schedule, in the limit as $T \\to \\infty$, the process converges to pure Gaussian noise $\\mathcal{N}(0, I)$.\n",
        "\n",
        "The forward distributions $q$ are chosen to be Gaussian:\n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I),\n",
        "\\quad \\quad\n",
        "q(x_{1:T} \\mid x_0) = \\prod_{t=1}^T q(x_t \\mid x_{t-1})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Reverse Diffusion Process\n",
        "\n",
        "The reverse process denoises noise until an object from the original data distribution is obtained.\n",
        "\n",
        "Thus, a diffusion model is a probabilistic latent variable model of the form:\n",
        "\n",
        "$$\n",
        "p_\\theta(x_0) := \\int p_\\theta(x_{0:T}) \\, dx_{1:T}\n",
        "$$\n",
        "\n",
        "where the intermediate states $x_1, \\dots, x_T$ correspond to noisy versions of the data, and $x_0$ is the clean sample.\n",
        "\n",
        "The joint distribution $p_\\theta(x_{0:T})$ is called the reverse diffusion process, which is also a Markov chain of Gaussian distributions:\n",
        "\n",
        "$$\n",
        "p(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t),\n",
        "\\quad \\quad\n",
        "p_\\theta(x_T) = \\mathcal{N}(x_T \\mid 0, I)\n",
        "$$\n",
        "\n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Efficient Sampling of the Forward Process\n",
        "\n",
        "Returning to $q(x_t \\mid x_{t-1})$:  \n",
        "naively, obtaining $x_t$ would require iteratively sampling $x_1, \\dots, x_{t-1}$.  \n",
        "However, due to the Gaussian structure, this can be done more efficiently.\n",
        "\n",
        "Define:\n",
        "$$\n",
        "\\alpha_t := 1 - \\beta_t,\n",
        "\\quad \\quad\n",
        "\\bar{\\alpha}_t := \\prod_{i=1}^t \\alpha_i\n",
        "$$\n",
        "\n",
        "Then:\n",
        "$$\n",
        "q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
        "\\quad \\quad \\quad (1)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Training Objective\n",
        "\n",
        "The model can be trained by optimizing individual terms of the variational lower bound of $\\log p_\\theta(x_0)$:\n",
        "\n",
        "$$\n",
        "L_{VLB} = \\mathbb{E}_q \\Big[\n",
        "\\underbrace{D_{\\text{KL}}(q(\\mathbf{x}_T \\mid \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T}\n",
        "+ \\sum_{t=2}^T \\underbrace{D_{\\text{KL}}(q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n",
        "\\parallel p_\\theta(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t))}_{L_{t-1}}\n",
        "\\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\mid \\mathbf{x}_1)}_{L_0}\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "To train the model, it is sufficient to write down:\n",
        "\n",
        "$$\n",
        "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)\n",
        "= \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I})\n",
        "$$\n",
        "\n",
        "with:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\mu}(\\mathbf{x}_t, \\mathbf{x}_0)\n",
        "= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t\n",
        "+ \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\n",
        "\\quad \\quad (2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\tilde{\\beta}_t\n",
        "= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n",
        "\\quad \\quad (3)\n",
        "$$\n",
        "For more details, see the paper  \n",
        "[Denoising Diffusion Probabilistic Models (Ho et al. 2020)](https://arxiv.org/abs/2006.11239).\n",
        "\n",
        "---\n",
        "\n",
        "### Simplified Training Objective\n",
        "\n",
        "The paper shows that better results are obtained by training with a simpler loss.\n",
        "\n",
        "Observe that:\n",
        "$$\n",
        "x_t(x_0, \\epsilon)\n",
        "= \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,\n",
        "\\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
        "\\quad \\quad (4)\n",
        "$$\n",
        "\n",
        "We let the model with parameters $\\theta$ predict the noise $\\epsilon$ in the equation above.\n",
        "\n",
        "The training objective becomes:\n",
        "$$\n",
        "L^{\\text{simple}}_t\n",
        "= \\mathbb{E}_{x_0, \\epsilon, t} \\big[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\big]\n",
        "$$\n",
        "\n",
        "The intuition is that the KL divergence between Gaussians is proportional to the squared difference between their means, and in this formulation the means are parameterized via $\\epsilon$.\n",
        "\n",
        "This is the loss you must use.\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling (Reverse Process)\n",
        "\n",
        "To sample from the model (reverse diffusion), we need to recover  \n",
        "$\\mu_\\theta(x_t, x_0)$ from $\\epsilon_\\theta(x_t, t)$.\n",
        "\n",
        "To do this:\n",
        "1. Compute $\\hat{x}_0(\\epsilon_\\theta, x_t)$ from equation (4)\n",
        "2. Substitute it into equation (2)\n",
        "\n",
        "This will be needed to implement `_predict_xstart_from_eps`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPkC73BJKHse"
      },
      "source": [
        "Moving on to the task. Here are two helper functions that we will later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9gHH9dPLY_-"
      },
      "outputs": [],
      "source": [
        "# some functions you will need\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "# utility function. basically, returns arr[timesteps], where timesteps are indices. (look at class Diffusion)\n",
        "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
        "    \"\"\"\n",
        "    Extract values from a 1-D torch tensor for a batch of indices.\n",
        "    :param arr: 1-D torch tensor.\n",
        "    :param timesteps: a tensor of indices into torch array to extract. (shape is [batch_size])\n",
        "    :param broadcast_shape: a larger shape of K dimensions; output shape will be broadcasted to this\n",
        "                            by adding new dimensions of size 1.\n",
        "                            the first dimension of output tensor will be equal to length of timesteps.\n",
        "    :return: a tensor of shape [batch_size, 1, ...] where tensor shape has K dims.\n",
        "    \"\"\"\n",
        "    res = arr.to(device=timesteps.device)[timesteps].float()\n",
        "    while len(res.shape) < len(broadcast_shape):\n",
        "        res = res[..., None]\n",
        "    return res.expand(broadcast_shape)\n",
        "\n",
        "\n",
        "# our beta_t. we use linear scheduler\n",
        "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
        "    \"\"\"\n",
        "    Get a pre-defined beta schedule for the given name.\n",
        "    The beta schedule library consists of beta schedules which remain similar\n",
        "    in the limit of num_diffusion_timesteps.\n",
        "    Beta schedules may be added, but should not be removed or changed once\n",
        "    they are committed to maintain backwards compatibility.\n",
        "    \"\"\"\n",
        "    scale = 1000 / num_diffusion_timesteps\n",
        "    beta_start = scale * 0.0001\n",
        "    beta_end = scale * 0.02\n",
        "    if schedule_name == \"linear\":\n",
        "        # Linear schedule from Ho et al, extended to work for any number of\n",
        "        # diffusion steps.\n",
        "        return np.linspace(\n",
        "            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
        "        )\n",
        "    elif schedule_name == \"quad\":\n",
        "        betas = (\n",
        "            torch.linspace(beta_start**0.5, beta_end**0.5, num_diffusion_timesteps) ** 2\n",
        "        )\n",
        "        return betas.numpy()\n",
        "    elif schedule_name == \"sigmoid\":\n",
        "        betas = torch.linspace(-6, 6, num_diffusion_timesteps)\n",
        "        betas = torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "        return betas.numpy()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFqaztHJXTx"
      },
      "source": [
        "### Diffusion Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXtIrJyGLZAA"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Diffusion:\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        betas: np.ndarray, # type: ignore\n",
        "        loss_type: str = \"mse\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Class that simulates Diffusion process. Does not store model or optimizer.\n",
        "        \"\"\"\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        betas = torch.from_numpy(betas).double()\n",
        "        self.betas = betas\n",
        "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
        "        assert (betas > 0).all() and (betas <= 1).all()\n",
        "\n",
        "        self.num_timesteps = int(betas.shape[0])\n",
        "\n",
        "        alphas = # todo\n",
        "        self.alphas_cumprod = # todo\n",
        "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]], dim=0)  # \\bar\\alpha_{t-1}\n",
        "        self.alphas_cumprod_next = torch.cat([self.alphas_cumprod[1:], torch.tensor([0.0]), ], dim=0)  # \\bar\\alpha_{t+1}\n",
        "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1})\n",
        "        self.sqrt_alphas_cumprod = self.alphas_cumprod.sqrt()\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n",
        "        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n",
        "        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n",
        "\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        self.posterior_variance = # todo, var from (3)\n",
        "\n",
        "        # log calculation clipped because posterior variance is 0.\n",
        "        self.posterior_log_variance_clipped = torch.log(\n",
        "            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]], dim=0)\n",
        "        )\n",
        "        self.posterior_mean_coef1 = # todo, coef of xt from (2)\n",
        "        self.posterior_mean_coef2 = # todo, coef of x0 from (2)\n",
        "\n",
        "    def q_mean_variance(self, x0, t):\n",
        "        \"\"\"\n",
        "        Get mean and variance of distribution q(x_t | x_0) for specific x_0 and t. Use equation (1).\n",
        "        \"\"\"\n",
        "\n",
        "        mean = # todo ; use _extract_into_tensor(*, t, x0.shape) function here and below for getting specific value from *alphas* array\n",
        "        variance = # todo\n",
        "        log_variance = # todo\n",
        "        return mean, variance, log_variance\n",
        "\n",
        "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
        "        \"\"\"\n",
        "        Compute mean and variance of diffusion posterior q(x_{t-1} | x_t, x_0) for specific x_t and t.\n",
        "        Use equation (2) and (3).\n",
        "\n",
        "        x_start is x_0 in formulas\n",
        "        \"\"\"\n",
        "        assert x_start.shape == x_t.shape\n",
        "        posterior_mean = # todo\n",
        "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
        "        posterior_log_variance_clipped = _extract_into_tensor(\n",
        "            self.posterior_log_variance_clipped, t, x_t.shape\n",
        "        )\n",
        "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        \"\"\"\n",
        "        Diffuse data for a given number of diffusion steps.\n",
        "        Sample from q(x_t | x_0) use (4).\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "        return # todo\n",
        "\n",
        "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
        "        \"\"\"\n",
        "        Get \\hat{x0} from epsilon_{theta}. Use equation (4) to derive it.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def p_mean_variance(self, model_output, x, t):\n",
        "        \"\"\"\n",
        "        Apply model to get p(x_{t-1} | x_t). Use Equation (2) and plug in \\hat{x}_0;\n",
        "        \"\"\"\n",
        "        model_variance = torch.cat([self.posterior_variance[1:2], self.betas[1:]], dim=0)\n",
        "        model_log_variance = torch.log(model_variance)\n",
        "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
        "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
        "\n",
        "        pred_xstart = self._predict_xstart_from_eps(x, t, model_output)\n",
        "\n",
        "        model_mean = # todo ; don't forget to extract specific values from posterior_mean_coef1 and posterior_mean_coef2 using _extract_into_tensor\n",
        "\n",
        "        return {\n",
        "            \"mean\": model_mean,\n",
        "            \"variance\": model_variance,\n",
        "            \"log_variance\": model_log_variance,\n",
        "            \"pred_xstart\": pred_xstart,\n",
        "        }\n",
        "\n",
        "    def p_sample(self, model_output, x, t):\n",
        "        \"\"\"\n",
        "        Sample from p(x_{t-1} | x_t).\n",
        "        \"\"\"\n",
        "        out = # todo; get mean, variance of p(xt-1|xt)\n",
        "        noise = torch.randn_like(x)\n",
        "        nonzero_mask = (\n",
        "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
        "        )  # no noise when t == 0\n",
        "\n",
        "\n",
        "        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n",
        "        return {\"sample\": sample}\n",
        "\n",
        "    def p_sample_loop(self, model, shape, y_dist):\n",
        "        \"\"\"\n",
        "        Samples a batch=shape[0] using diffusion model.\n",
        "        \"\"\"\n",
        "\n",
        "        x = torch.randn(*shape, device=model.device)\n",
        "        indices = list(range(self.num_timesteps))[::-1]\n",
        "\n",
        "        y = torch.multinomial(\n",
        "            y_dist,\n",
        "            num_samples=shape[0],\n",
        "            replacement=True\n",
        "        ).to(x.device)\n",
        "\n",
        "        for i in tqdm(indices):\n",
        "            t = torch.tensor([i] * shape[0], device=x.device)\n",
        "            with torch.no_grad():\n",
        "                model_output = model(x, t, y)\n",
        "                out = self.p_sample(\n",
        "                    model_output,\n",
        "                    x,\n",
        "                    t\n",
        "                )\n",
        "                x = out[\"sample\"]\n",
        "        return x, y\n",
        "\n",
        "    def train_loss(self, model, x0, y):\n",
        "        \"\"\"\n",
        "        Calculates loss L^{simple}_t for the given model, x0.\n",
        "        \"\"\"\n",
        "        t = torch.randint(0, self.num_timesteps, size=(x0.size(0),), device=x0.device)\n",
        "        noise = # todo: sample tensor of shape x0 with noise (from std normal distribution)\n",
        "        x_t = # todo use q_sample() to get diffused samples with specific noise\n",
        "        model_output = # todo; predict sampled noise from (x_t, t, y)\n",
        "        if self.loss_type == 'mse':\n",
        "            loss = # todo; compute mse loss\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ4Mi-mlLZAB"
      },
      "outputs": [],
      "source": [
        "T = 100\n",
        "\n",
        "diffusion = Diffusion(betas=get_named_beta_schedule(\"linear\", T), loss_type=\"mse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dias6xFZEeSH"
      },
      "outputs": [],
      "source": [
        "# Check some coeffs (beta version asserts, but they should help)\n",
        "assert torch.allclose(\n",
        "    diffusion.alphas_cumprod[[0, 5, 60]],\n",
        "    torch.DoubleTensor([0.999, 0.9644, 0.0202]),\n",
        "    rtol=1e-4,\n",
        "    atol=1e-4,\n",
        ")\n",
        "\n",
        "\n",
        "assert torch.allclose(\n",
        "    diffusion.posterior_mean_coef2[[2, 20, 60]],\n",
        "    torch.DoubleTensor([0.5562, 0.0928, 0.0188]),\n",
        "    rtol=1e-4,\n",
        "    atol=1e-4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6uoyhHtEeSI"
      },
      "outputs": [],
      "source": [
        "# Check some methods (beta version asserts, but they should help)\n",
        "assert torch.allclose(\n",
        "    diffusion.q_sample(\n",
        "        x_start=torch.FloatTensor([[0.1, -0.2, 0.3]]),\n",
        "        t=torch.LongTensor([[42]]),\n",
        "        noise=torch.FloatTensor([[0.01, -0.02, 0.1]]),\n",
        "    )[0],\n",
        "    torch.FloatTensor([[0.0476, -0.0953, 0.2075]]),\n",
        "    rtol=1e-4,\n",
        "    atol=1e-4,\n",
        ")\n",
        "\n",
        "assert torch.allclose(\n",
        "    diffusion.p_mean_variance(\n",
        "        model_output=torch.FloatTensor([[0.01, -0.21, 0.32]]),\n",
        "        x=torch.FloatTensor([[0.01, -0.02, 0.1]]),\n",
        "        t=torch.LongTensor([[42]]),\n",
        "    )[\"mean\"][0],\n",
        "    torch.FloatTensor([[0.0095, -0.0006, 0.0736]]),\n",
        "    rtol=1e-4,\n",
        "    atol=1e-4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvZykOR5LZAB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def show_noising(diffusion, X, y):\n",
        "    fig, axs = plt.subplots(1, 10, figsize=(40, 5))\n",
        "    for i, t in enumerate(range(0, diffusion.num_timesteps, 10)):\n",
        "        x = diffusion.q_sample(\n",
        "            x_start=torch.from_numpy(X),\n",
        "            t=torch.ones_like(torch.from_numpy(y)).long() * t,\n",
        "        )\n",
        "\n",
        "        sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, ax=axs[i])\n",
        "        axs[i].set(title=t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKnZBT70LZAC"
      },
      "source": [
        "Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼, ÐºÐ°Ðº Ð·Ð°ÑˆÑƒÐ¼Ð»ÑÑŽÑ‚ÑÑ Ð½Ð°ÑˆÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸ÐµÐ¼ $t$. ÐšÐ°Ðº Ð´ÑƒÐ¼Ð°ÐµÑ‚Ðµ, Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð»Ð¸ $T = 100$ Ð¸Ð»Ð¸ Ð½Ð°Ð´Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ? ÐšÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾Ð½ÑÑ‚ÑŒ, ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾? (Ð½Ðµ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ÑÑ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AfcaKEeLZAC"
      },
      "outputs": [],
      "source": [
        "show_noising(diffusion, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22OaGgtLZAE"
      },
      "source": [
        "### ÐœÐ¾Ð´ÐµÐ»ÑŒ (1 Ð±Ð°Ð»Ð» Ð¸Ð· 7 Ð·Ð° DDPM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRw1RGKLZAE"
      },
      "source": [
        "Ð¢ÑƒÑ‚ Ð¼Ñ‹ Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ Ð²ÐµÑÐ°Ð¼Ð¸ $\\theta$, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸Ð·ÑƒÐµÑ‚ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ ÑÐ»Ð¾Ð¶Ð½Ð¾Ð¹ Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹. Ð”Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð»Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ñ… ÑÐ»Ð¾ÐµÐ². ÐÐµ Ð·Ð°Ð±ÑƒÐ´ÑŒÑ‚Ðµ ÑƒÑ‡ÐµÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑÑ‹ $y$ Ð¸ ÑˆÐ°Ð³Ð¸ $t$. ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑˆÑƒÐ¼ $\\epsilon: \\epsilon_{\\theta}(x_t, t, y)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSYC7KmvLZAF"
      },
      "outputs": [],
      "source": [
        "class DiffModel(nn.Module):\n",
        "    def __init__(self, d_in, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden = 128\n",
        "\n",
        "        # one may use a simple model that projects x and t into space of size self.hidden\n",
        "        # transforms y label into space of size self.hidden (nn.Embedding), sum all the vectors and postprocess it with MLP\n",
        "        # try SiLU?? stonks\n",
        "\n",
        "        self.x_proj = # todo\n",
        "        self.t_proj = # todo\n",
        "        self.y_embed = # todo\n",
        "        self.layers = # todo\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        '''\n",
        "        :x input, e.g. images\n",
        "        :t 1d torch.LongTensor of timesteps\n",
        "        :y 1d torch.LongTensor of class labels\n",
        "        '''\n",
        "        # todo\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIuhHwTLZAG"
      },
      "source": [
        "### ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (1 Ð±Ð°Ð»Ð» Ð¸Ð· 7 Ð·Ð° DDPM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfjHOndQLZAG"
      },
      "source": [
        "ÐÐ°ÐºÐ¾Ð½ÐµÑ†, Ð¾Ð±ÑƒÑ‡Ð¸Ð¼ Ð½Ð°ÑˆÑƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ. ÐÐ¸Ð¶Ðµ Ð·Ð° Ð²Ð°Ñ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½ ÐºÐ»Ð°ÑÑ `Trainer`, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ, Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸ÑŽ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð¹Ð·ÐµÑ€. Ð’Ð°Ð¼ Ð½Ð°Ð´Ð¾ Ð»Ð¸ÑˆÑŒ Ð´Ð¾Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ `_run_step`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl6OajqTLZAG"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion: Diffusion,\n",
        "        model: nn.Module,\n",
        "        train_iter,  # iterable that yields (x, y)\n",
        "        lr: float,\n",
        "        weight_decay: float,\n",
        "        steps: int,\n",
        "        device: torch.device = torch.device(\"cpu\"),\n",
        "    ):\n",
        "        self.diffusion = diffusion\n",
        "\n",
        "        self.train_iter = train_iter\n",
        "        self.steps = steps\n",
        "        self.init_lr = lr\n",
        "        self.model = model\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(), lr=lr, weight_decay=weight_decay\n",
        "        )\n",
        "        self.device = device\n",
        "        self.log_every = 100\n",
        "        self.print_every = 500\n",
        "\n",
        "    def _anneal_lr(self, step: int):\n",
        "        \"\"\"\n",
        "        Performs annealing of lr.\n",
        "        \"\"\"\n",
        "\n",
        "        frac_done = step / self.steps\n",
        "        lr = self.init_lr * (1 - frac_done)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "    def _run_step(self, x: torch.FloatTensor, y: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        A single training step.\n",
        "        Calculates loss (using Diffusion.train_loss() )for a single batch.\n",
        "        Then performs a single optimizer step (don't forget to zero grad) and returns loss.\n",
        "        \"\"\"\n",
        "        # todo\n",
        "        # zero_grad, calc loss, backw\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def run_loop(self):\n",
        "        \"\"\"\n",
        "        Training loop.\n",
        "        \"\"\"\n",
        "        step = 0\n",
        "        curr_loss_gauss = 0.0\n",
        "\n",
        "        curr_count = 0\n",
        "        while step < self.steps:\n",
        "            x, y = next(self.train_iter)\n",
        "            batch_loss = self._run_step(x, y)\n",
        "\n",
        "            self._anneal_lr(step)\n",
        "\n",
        "            curr_count += len(x)\n",
        "            curr_loss_gauss += batch_loss.item() * len(x)\n",
        "\n",
        "            if (step + 1) % self.log_every == 0:\n",
        "                gloss = np.around(curr_loss_gauss / curr_count, 4)\n",
        "                if (step + 1) % self.print_every == 0:\n",
        "                    print(f\"Step {(step + 1)}/{self.steps} Loss: {gloss}\")\n",
        "                curr_count = 0\n",
        "                curr_loss_gauss = 0.0\n",
        "\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOh210qELZAG"
      },
      "source": [
        "Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¾Ð±ÐµÑ€Ð½ÐµÐ¼ Ð½Ð°ÑˆÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² `DataLoader`. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ð·Ð° Ð²Ð°Ñ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½ `FastTensorDataLoader`. Ð¢Ð°ÐºÐ¶Ðµ Ñƒ Ð½Ð°Ñ Ð¸Ð´ÐµÑ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ðµ Ð¿Ð¾ ÑÐ¿Ð¾Ñ…Ð°Ð¼, Ð° Ð¿Ð¾ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð½Ð°Ð¼ Ð½ÑƒÐ¶ÐµÐ½ \"Ð±ÐµÑÐºÐ¾Ð½ÐµÑ‡Ð½Ñ‹Ð¹\" Ð¸Ñ‚ÐµÑ€Ð°Ñ‚Ð¾Ñ€."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a6yQGSTLZAG"
      },
      "outputs": [],
      "source": [
        "from utils import FastTensorDataLoader\n",
        "\n",
        "\n",
        "def get_data_iter(X: np.ndarray, y: np.ndarray, batch_size: int = 512):\n",
        "    X = torch.from_numpy(X).float()\n",
        "    y = torch.from_numpy(y).long()\n",
        "    dataloader = FastTensorDataLoader(X, y, batch_size=batch_size, shuffle=True)\n",
        "    while True:\n",
        "        yield from dataloader\n",
        "\n",
        "\n",
        "data_iter = get_data_iter(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr-YGEIuLZAH"
      },
      "outputs": [],
      "source": [
        "# you can change hyperparameters\n",
        "model = DiffModel(d_in=2)\n",
        "model.device = torch.device(\"cpu\")  # Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ cpu\n",
        "trainer = Trainer(\n",
        "    diffusion, model, train_iter=data_iter, lr=0.01, weight_decay=0.0, steps=6000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0dtk6JaLZAH"
      },
      "outputs": [],
      "source": [
        "trainer.run_loop()  # < 1min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf5sPAxILZAH"
      },
      "source": [
        "Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð½Ð°ÑÑÐ¼Ð¿Ð»Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð½Ð°ÑˆÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y552va6ELZAH"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def sample_synthetic(\n",
        "    diffusion: Diffusion,\n",
        "    model: nn.Module,\n",
        "    num_samples: int,\n",
        "    batch: int = 1000,\n",
        "    shape: Tuple = (2,),\n",
        "    y_dist: List[float] = [0.5, 0.5],\n",
        "    ddim: bool = False,\n",
        "):\n",
        "    sample_func = diffusion.p_sample_loop\n",
        "    if ddim:  # for the last task\n",
        "        sample_func = diffusion.ddim_sample\n",
        "    res_x = []\n",
        "    res_y = []\n",
        "    num_sampled = 0\n",
        "    while num_sampled < num_samples:\n",
        "        x, y = diffusion.p_sample_loop(\n",
        "            model, shape=(batch, *shape), y_dist=torch.tensor(y_dist)\n",
        "        )\n",
        "        res_x.append(x.cpu())\n",
        "        res_y.append(y.cpu())\n",
        "        num_sampled += batch\n",
        "\n",
        "    res_x = torch.cat(res_x, dim=0)\n",
        "    res_y = torch.cat(res_y, dim=0)\n",
        "    return res_x[:num_samples], res_y[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCYZQbfILZAI"
      },
      "outputs": [],
      "source": [
        "Xs, ys = sample_synthetic(diffusion, model, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMDKIkc5LZAI"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=Xs[:, 0], y=Xs[:, 1], hue=ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSS_RMuuLZAI"
      },
      "source": [
        "ÐžÑ†ÐµÐ½Ð¸Ñ‚Ðµ Ð½Ð° Ð³Ð»Ð°Ð·, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð»Ð¾ÑÑŒ (Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ Ð½ÐµÐ¿Ð»Ð¾Ñ…Ð¾). ÐšÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¾ Ð¾Ñ†ÐµÐ½Ð¸Ñ‚ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð½Ð°ÑÑÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¸Ð¼ÐµÐ½Ð½Ð¾ ÑÑ‚Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…)?  ÐŸÑƒÐ½ÐºÑ‚ Ð½Ðµ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ÑÑ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoU_XRKCLBQJ"
      },
      "source": [
        "ÐŸÐ¾ÐºÐ°Ð¶Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ñ€Ð°ÑÑˆÑƒÐ¼Ð»ÐµÐ½Ð¸Ñ Ð°Ð½Ð°Ð»Ð°Ð³Ð¸Ñ‡Ð½Ð¾ Ñ‚Ð¾Ð¼Ñƒ, ÐºÐ°Ðº Ð¼Ñ‹ ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°Ð»Ð¸ Ð´Ð»Ñ Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdA95T6bLKdf"
      },
      "outputs": [],
      "source": [
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_KJR7vLLZAJ"
      },
      "source": [
        "## MNIST (2 Ð±Ð°Ð»Ð»Ð°)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl6fIqYkLZAJ"
      },
      "source": [
        "ÐŸÐµÑ€ÐµÐ¹Ð´Ñ‘Ð¼ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° MNIST. Ð—Ð° Ð²Ð°Ñ ÑƒÐ¶Ðµ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð° Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸. Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ðµ Ð½Ð°Ð´Ð¾ Ð»Ð¸ÑˆÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ *Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹* Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð½Ð° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÐºÐ»Ð°ÑÑ `Diffusion`, Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ñ ÑˆÑƒÐ¼Ð°."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMz3KEuxLC5K"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class InfiniteDataLoader(DataLoader):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Initialize an iterator over the dataset.\n",
        "        self.dataset_iterator = super().__iter__()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            batch = next(self.dataset_iterator)\n",
        "        except StopIteration:\n",
        "            # Dataset exhausted, use a new fresh iterator.\n",
        "            self.dataset_iterator = super().__iter__()\n",
        "            batch = next(self.dataset_iterator)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFiFelJwLZAK"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision.transforms import Compose, Lambda, Normalize, ToTensor\n",
        "\n",
        "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "dataset = MNIST(\"./datasets\", download=True, train=True, transform=transform)\n",
        "loader = InfiniteDataLoader(dataset, 512, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej70iAHKLZAK"
      },
      "outputs": [],
      "source": [
        "def show_images(images, ys, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "\n",
        "    # Converting images to CPU numpy arrays\n",
        "    if type(images) is torch.Tensor:\n",
        "        images = images.detach().cpu().numpy()\n",
        "        ys = ys.detach().cpu().numpy()\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
        "                plt.title(f\"{int(ys[idx])}\")\n",
        "                plt.tick_params(bottom=False, labelbottom=False)\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "\n",
        "    # Showing the figure\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_first_batch(loader):\n",
        "    for batch in loader:\n",
        "        show_images(batch[0][:16], batch[1][:16], \"Images in the first batch\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvHVOBxKLZAL"
      },
      "outputs": [],
      "source": [
        "show_first_batch(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymS7VXuALZAL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\n",
        "    f\"Using device: {device}\\t\"\n",
        "    + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6UXgAnbLZAM"
      },
      "outputs": [],
      "source": [
        "from utils import MyUNet\n",
        "\n",
        "model_mnist = MyUNet().to(device)\n",
        "model_mnist.device = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWdlIb3LZAM"
      },
      "source": [
        "### Ð¡Ð¼Ð¾Ñ‚Ñ€Ð¸Ð¼ Ð½Ð° Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ ÑˆÑƒÐ¼Ð°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PlNaBRVLZAM"
      },
      "source": [
        "ÐŸÑ€ÐµÐ¶Ð´Ðµ Ñ‡ÐµÐ¼ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ, Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹Ñ‚Ðµ Ð½Ð° Ð¾Ð´Ð½Ð¾Ð¹ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐµ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¸ $\\sqrt{\\bar{\\alpha_t}}$ (Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ $t$) Ð´Ð»Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ð¹ (linear, quad, sigmoid). ÐžÐ±ÑŠÑÑÐ½Ð¸Ñ‚Ðµ, Ñ‡ÐµÐ¼ Ð¾Ð½Ð¸ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð°ÑŽÑ‚ÑÑ. Ð§Ñ‚Ð¾Ð±Ñ‹ Ð»ÑƒÑ‡ÑˆÐµ ÑÑ‚Ð¾ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð·Ð°ÑˆÑƒÐ¼Ð»ÐµÐ½Ð¸Ðµ ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº (Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ `show_noising_mnist`). Ð¡Ð¾Ð²ÐµÑ‚ÑƒÑŽ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ $T = 1000$ (ÐºÐ»Ð°ÑÑÐ¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€, ÐµÑÐ»Ð¸ Ð´Ð¾Ð¼ÐµÐ½ -- ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸). Ð”Ð°Ð½Ð½Ñ‹Ð¹ Ð¿ÑƒÐ½ÐºÑ‚ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ÑÑ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt5z549FLZAN"
      },
      "outputs": [],
      "source": [
        "def get_alpha_bar(betas):\n",
        "    # todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpRDr6f5LZAN"
      },
      "outputs": [],
      "source": [
        "ts = range(1000)\n",
        "\n",
        "# your plots here\n",
        "\n",
        "plt.legend([\"linear\", \"quad\", \"sigmoid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RTnAXNmLZAN"
      },
      "outputs": [],
      "source": [
        "# almost the same as in the task with SwissRolls\n",
        "\n",
        "def show_noising_mnist(diffusion, img):\n",
        "   # todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OklbFbrLZAN"
      },
      "outputs": [],
      "source": [
        "for sch in [\"linear\", \"quad\", \"sigmoid\"]:\n",
        "    diffusion_temp = Diffusion(\n",
        "        betas=get_named_beta_schedule(sch, 1000), loss_type=\"mse\"\n",
        "    )\n",
        "    show_noising_mnist(diffusion_temp, dataset[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWJNsaX7LZAO"
      },
      "source": [
        "### ÐžÐ±ÑƒÑ‡Ð°ÐµÐ¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1ywU_RGLZAO"
      },
      "outputs": [],
      "source": [
        "scheduler = # choose your pokemon\n",
        "\n",
        "diffusion_mnist = Diffusion(\n",
        "    betas=get_named_beta_schedule(scheduler, 1000),\n",
        "    loss_type=\"mse\"\n",
        ")\n",
        "\n",
        "# feel free to change hyperaparameters\n",
        "\n",
        "trainer_mnist = Trainer(\n",
        "    diffusion_mnist,\n",
        "    model_mnist,\n",
        "    train_iter=loader,\n",
        "    lr=0.001,\n",
        "    steps=1000,\n",
        "    weight_decay=0.0,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9iksepULZAP"
      },
      "outputs": [],
      "source": [
        "trainer_mnist.run_loop()  # <15min on 2080Ti for the author's solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge0P-sUwLZAP"
      },
      "outputs": [],
      "source": [
        "Xs, ys = sample_synthetic(\n",
        "    diffusion_mnist,\n",
        "    model_mnist,\n",
        "    num_samples=16,\n",
        "    batch=16,\n",
        "    shape=(1, 28, 28),\n",
        "    y_dist=[0.1 for _ in range(10)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlmccDpsLZAW"
      },
      "outputs": [],
      "source": [
        "show_images(Xs, ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33pasGULZAX"
      },
      "source": [
        "ÐžÑ†ÐµÐ½Ð¸Ñ‚Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð½Ð°ÑÑÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybrMTWWSslI"
      },
      "source": [
        "## Ð‘Ð¾Ð½ÑƒÑ (2 Ð±Ð°Ð»Ð»)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io_DOhMoSslI"
      },
      "source": [
        "Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¾Ð±ÑƒÑ‡Ð¸Ñ‚Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¾Ð½ÐºÑƒ Ð½Ð° Ñ†Ð²ÐµÑ‚Ð½Ð¾Ð¼ MNIST. ÐŸÐ¾ [ÑÑÑ‹Ð»ÐºÐµ](https://disk.yandex.ru/d/D_7g3UDaYXK4Lg) Ð¸Ð· Ð´Ð¾Ð¼Ð°ÑˆÐºÐ¸ Ñ ÐºÑƒÑ€ÑÐ° \"Ð“ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð´Ð¸Ñ„Ñ„ÑƒÐ·Ð¸Ð¸\" Ð”ÐµÐ½Ð¸ÑÐ° Ð Ð°ÐºÐ¸Ñ‚Ð¸Ð½Ð° Ð² Ñ„Ð°Ð¹Ð»Ðµ HW1-1.ipynb Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð·ÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ Ñ†Ð²ÐµÑ‚Ð½Ð¾Ð³Ð¾ MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxuZCoYSslI"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8QeyuRXSslJ"
      },
      "source": [
        "## ÐžÑ‚Ñ‡Ñ‘Ñ‚\n",
        "```(Ð±ÐµÐ· ÑÑ‚Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¸ 0 Ð·Ð° Ð´Ð·)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlxpdoA5SslJ"
      },
      "source": [
        "ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð´ÐµÐ»Ð°Ð½Ð½Ð¾Ð¹ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ (Ñ‚ÑƒÑ‚ Ð¼Ð¾Ð¶Ð½Ð¾ Ð½Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð²ÑÐµ, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ Ðº Ð´Ð¾Ð¼Ð°ÑˆÐºÐµ Ð¸ Ð½Ðµ Ð·Ð°Ð±ÑƒÐ´ÑŒÑ‚Ðµ ÑÑÑ‹Ð»ÐºÑƒ Ð½Ð° wandb Ð¸Ð»Ð¸ Ð½Ð° Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ñ‹Ð¹ ÑÐµÑ€Ð²Ð¸Ñ).\n",
        "\n",
        "```[ÐžÑ‚Ñ‡ÐµÑ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ñ‚ÑƒÑ‚]```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZrbco_QSslJ"
      },
      "source": [
        "## ðŸ‘€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQZCvUzSslK"
      },
      "source": [
        "Ð’ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ Ð»ÑŽÐ±Ð¾Ð¹ Ð¼ÐµÐ¼ Ð¸Ð»Ð¸ Ð°Ð½ÐµÐºÐ´Ð¾Ñ‚, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð´Ð½ÑÑ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÑŽÑ‰ÐµÐ¼Ñƒ (ÐµÑÐ»Ð¸ Ð±ÑƒÐ´ÐµÑ‚ ÑÐ¼ÐµÑˆÐ½Ð¾Ð¹, Ñ‚Ð¾ Ð±Ð¾Ð½ÑƒÑ ```+0.05 Ð±Ð°Ð»Ð»Ð°```)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a06af253165e97d0c1e75e8bf6d3252013856f30b8177e11b02d3fa36c37333d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}